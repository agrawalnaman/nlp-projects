{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation using a Sensegram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Initializations\n",
    "\n",
    "We need to import `numpy` for working with arrays, and other libs like `os`, `pickle` and `pprint` for other utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pprint, pickle, re\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "\n",
    "lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "TEST_SENTENCES_PATH = '/Users/sounak/Documents/clg/nlp/nlp-projects/data/wsd/sentences.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "The two helper functions `save_obj` and `load_obj` are used to pickle any object and load back the pickle file. These functions will be useful in saving the vector dicts and thus faster loading of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    if 'obj' not in os.listdir():\n",
    "        os.mkdir('obj')\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    try:\n",
    "        with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Sensegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sense_vecs have been loaded\n"
     ]
    }
   ],
   "source": [
    "sense_vecs = load_obj('sense_vecs')\n",
    "pos_tags = load_obj('pos_tags')\n",
    "\n",
    "if not (sense_vecs and pos_tags):\n",
    "    SENSEGRAM_PATH = \"/Users/sounak/Documents/clg/nlp/nlp-projects/data/sensegrams_of_wikipedia_cluster\"\n",
    "    f = open(SENSEGRAM_PATH, 'r')\n",
    "    sense_vecs = {}\n",
    "    pos_tags = set()\n",
    "\n",
    "    for line in f.readlines():\n",
    "        t = line.split('\\t')\n",
    "        word, pos = t[0].split('#')\n",
    "        pos_tags.add(pos)\n",
    "        if t[1] == '0':\n",
    "            sense_vecs[(word, pos)] = []\n",
    "        sense_vecs[(word, pos)].append(np.array(eval(t[2])))\n",
    "    f.close()\n",
    "    save_obj(sense_vecs, 'sense_vecs')\n",
    "    save_obj(pos_tags, 'pos_tags')\n",
    "\n",
    "print('sense_vecs have been loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Glove Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_vecs have been loaded\n"
     ]
    }
   ],
   "source": [
    "word_vecs = load_obj('word_vecs')\n",
    "\n",
    "if not word_vecs:\n",
    "    GLOVE_PATH = \"/Users/sounak/Documents/clg/nlp/nlp-projects/data/glove.6B.300d.txt\"\n",
    "    f = open(GLOVE_PATH, 'r')\n",
    "    word_vecs = {}\n",
    "    for line in f.readlines():\n",
    "        t = line.split(' ')\n",
    "        word_vecs[t[0]] = np.array([float(_) for _ in t[1:]])\n",
    "    f.close()\n",
    "    save_obj(word_vecs, 'word_vecs')\n",
    "    \n",
    "print('word_vecs have been loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Sense\n",
    "\n",
    "The function `compute_sense_idx` takes a sentence, the target and some other arguments and returns the index of the sense of the target that was used in the current context.\n",
    "\n",
    "This function maximizes the cosine similarity of an aggregate context vector with the vectors of the different senses of the target word. It also doesn't include the stop words in the context. The aggregate context vector is calculated using the lemmatized words in the context after removing the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('en')\n",
    "\n",
    "def compute_sense_idx(sentence, target):\n",
    "    if target not in sentence:\n",
    "        return None\n",
    "    sentence = nltk.pos_tag(sentence)\n",
    "    context = list(filter(lambda x: x[0] != target, sentence))\n",
    "    sum = np.zeros(300)\n",
    "    context_final = [(lem.lemmatize(w, pos[0].lower()), pos) for w, pos in context if w not in stop_words]\n",
    "    print(context_final)\n",
    "    for w, _ in context_final:\n",
    "        sum += word_vecs[w]\n",
    "        \n",
    "    cw_mean = np.divide(sum, len(context))\n",
    "    max_idx = -1\n",
    "    max_value = float('-inf')\n",
    "    for pos in pos_tags:\n",
    "        try:\n",
    "            for idx, sense in enumerate(sense_vecs[(target, pos)]):\n",
    "                if np.linalg.norm(sense) > 0:\n",
    "                    result = np.divide(np.dot(sense, cw_mean), (np.linalg.norm(sense) * np.linalg.norm(cw_mean)))\n",
    "                    if result > max_value:\n",
    "                        max_value = result\n",
    "                        max_idx = idx\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return max_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "This is a light-weight tokenizer for tokenizing the input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    words = [_.lower() for _ in re.split(r\"[^a-zA-ZÀ-ÿ0-9']+\", text)]\n",
    "    return list(filter(('').__ne__, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "The sentences must provided in the following way, `<sentence>/<target_word>` in every line of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('go', 'VBG'), ('director', 'NN'), ('project', 'NN')]\n",
      "1\n",
      "[('tie', 'VBN'), ('tightly', 'RB'), ('ball', 'NN')]\n",
      "0\n",
      "[('need', 'VBP'), ('tie', 'VB'), ('cow', 'NN'), ('pole', 'NN')]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "f = open(TEST_SENTENCES_PATH, 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "for line in lines:\n",
    "    sentence, target = line.split('/')\n",
    "    sentence = tokenize(sentence)\n",
    "    target = target.strip()\n",
    "    print(compute_sense_idx(sentence, target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
