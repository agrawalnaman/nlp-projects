{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation using a Sensegram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "\n",
    "### Sense Determination\n",
    "\n",
    "We calculate the sense of a target word in a particular context by maximizing the cosine similarity between the aggregate context vector (average of the context word vectors after removing the stop words) and the different sense vectors of the target word.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "For evaluation, we check the key to **group** all the sentences in the test data which have the same sense for the same target word. Then we run the function on all these sentences (of a *group*) to check whether most of them (ideally all of them) have the same index or not.\n",
    "\n",
    "On running the function on all these sentences (of a *group*) we get the sense indices. We are making an assumption here, that is, the most common sense index that we are obtaining is the correct sense index for this *group* of sentences. Then the measure of accuracy is calculated using the formula:\n",
    "\n",
    "```example\n",
    "accuracy = ∑(g) #(most_common_index(g)) / total_sentences\n",
    "```\n",
    "\n",
    "where `#(most_common_index(g))` gives the number of occurences of the most common index on running the function on a *group* `g` and `total_sentences` is the total number of sentences in the test dataset which give a valid output on running the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Initializations\n",
    "\n",
    "We need to import `numpy` for working with arrays, and other libs like `os`, `pickle` and `pprint` for other utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pprint, pickle, re\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "\n",
    "lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "TEST_SENTENCES_PATH = '/Users/sounak/Documents/clg/nlp/nlp-projects/data/wsd/sentences.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "The two helper functions `save_obj` and `load_obj` are used to pickle any object and load back the pickle file. These functions will be useful in saving the vector dicts and thus faster loading of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    if 'obj' not in os.listdir():\n",
    "        os.mkdir('obj')\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    try:\n",
    "        with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Sensegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sense_vecs have been loaded\n"
     ]
    }
   ],
   "source": [
    "sense_vecs = load_obj('sense_vecs')\n",
    "pos_tags = load_obj('pos_tags')\n",
    "\n",
    "if not (sense_vecs and pos_tags):\n",
    "    SENSEGRAM_PATH = \"/Users/sounak/Documents/clg/nlp/nlp-projects/data/sensegrams_of_wikipedia_cluster\"\n",
    "    f = open(SENSEGRAM_PATH, 'r')\n",
    "    sense_vecs = {}\n",
    "    pos_tags = set()\n",
    "\n",
    "    for line in f.readlines():\n",
    "        t = line.split('\\t')\n",
    "        word, pos = t[0].split('#')\n",
    "        pos_tags.add(pos)\n",
    "        if t[1] == '0':\n",
    "            sense_vecs[(word, pos)] = []\n",
    "        sense_vecs[(word, pos)].append(np.array(eval(t[2])))\n",
    "    f.close()\n",
    "    save_obj(sense_vecs, 'sense_vecs')\n",
    "    save_obj(pos_tags, 'pos_tags')\n",
    "\n",
    "print('sense_vecs have been loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Glove Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_vecs have been loaded\n"
     ]
    }
   ],
   "source": [
    "word_vecs = load_obj('word_vecs')\n",
    "\n",
    "if not word_vecs:\n",
    "    GLOVE_PATH = \"/Users/sounak/Documents/clg/nlp/nlp-projects/data/glove.6B.300d.txt\"\n",
    "    f = open(GLOVE_PATH, 'r')\n",
    "    word_vecs = {}\n",
    "    for line in f.readlines():\n",
    "        t = line.split(' ')\n",
    "        word_vecs[t[0]] = np.array([float(_) for _ in t[1:]])\n",
    "    f.close()\n",
    "    save_obj(word_vecs, 'word_vecs')\n",
    "    \n",
    "print('word_vecs have been loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Sense\n",
    "\n",
    "The function `compute_sense_idx` takes a sentence, the target and some other arguments and returns the index of the sense of the target that was used in the current context.\n",
    "\n",
    "This function maximizes the cosine similarity of an aggregate context vector with the vectors of the different senses of the target word. It also doesn't include the stop words in the context. The aggregate context vector is calculated using the lemmatized words in the context after removing the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('en')\n",
    "\n",
    "def compute_sense_idx(sentence, target):\n",
    "    if target not in sentence:\n",
    "        return None\n",
    "    sentence = nltk.pos_tag(sentence)\n",
    "    context = list(filter(lambda x: x[0] != target, sentence))\n",
    "    sum = np.zeros(300)\n",
    "    preprocess = lambda w, pos : (lem.lemmatize(w, pos[0].lower()), pos) if pos[0].lower() in ['a', 'r', 'n', 'v'] else (w, pos)\n",
    "    context_final = [preprocess(w, pos) for w, pos in context if w not in stop_words]\n",
    "    for w, _ in context_final:\n",
    "        try:\n",
    "            sum += word_vecs[w]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "    cw_mean = np.divide(sum, len(context))\n",
    "    max_idx = -1\n",
    "    max_value = float('-inf')\n",
    "    for pos in pos_tags:\n",
    "        try:\n",
    "            for idx, sense in enumerate(sense_vecs[(target, pos)]):\n",
    "                if np.linalg.norm(sense) > 0:\n",
    "                    result = np.divide(np.dot(sense, cw_mean), (np.linalg.norm(sense) * np.linalg.norm(cw_mean)))\n",
    "                    if result > max_value:\n",
    "                        max_value = result\n",
    "                        max_idx = idx\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return max_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "This is a light-weight tokenizer for tokenizing the input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    words = [_.lower() for _ in re.split(r\"[^a-zA-ZÀ-ÿ0-9']+\", text)]\n",
    "    words = [_[:-2] if \"'s\" in _ else _ for _ in words]\n",
    "    return list(filter(('').__ne__, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "We are testing the function on the SemEval Test Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test sentences have been loaded\n"
     ]
    }
   ],
   "source": [
    "from xml.dom.minidom import parse\n",
    "FILE = './data/wsd-test/contexts/senseval2-format/semeval-2013-task-13-test-data.senseval2.xml'\n",
    "\n",
    "dom = parse(FILE)\n",
    "inst = dom.getElementsByTagName('instance')\n",
    "\n",
    "sents = {}\n",
    "\n",
    "for i in inst:\n",
    "    k = i.attributes['id'].value\n",
    "    context = i.getElementsByTagName('context')[0]\n",
    "    word = context.getElementsByTagName('head')[0].childNodes[0].nodeValue\n",
    "    v = ' {} '.format(word).join(t.nodeValue.strip() for t in context.childNodes if t.nodeType == t.TEXT_NODE)\n",
    "    sents[k + '.' + word] = tokenize(v)\n",
    "\n",
    "print('test sentences have been loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results have been loaded\n"
     ]
    }
   ],
   "source": [
    "res = {}\n",
    "\n",
    "for k, v in sents.items():\n",
    "    res['.'.join(k.split('.')[:-1])] = compute_sense_idx(v, k.split('.')[-1])\n",
    "    \n",
    "print('results have been loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys have been loaded\n"
     ]
    }
   ],
   "source": [
    "KEY = './data/wsd-test/keys/gold/all.singlesense.key'\n",
    "\n",
    "keys = {}\n",
    "keys_rev = {}\n",
    "f = open(KEY, 'r')\n",
    "for line in f.readlines():\n",
    "    l = line.strip().split(' ')\n",
    "    keys[l[1]] = l[2].split(':')[0].split('%')[1]\n",
    "    try:\n",
    "        keys_rev[l[0] + '%' + l[2].split(':')[0].split('%')[1]].append(l[1])\n",
    "    except KeyError:\n",
    "        keys_rev[l[0] + '%' + l[2].split(':')[0].split('%')[1]] = [l[1]]\n",
    "    \n",
    "print('keys have been loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.84126496776175\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "total = correct = 0\n",
    "\n",
    "for k in keys_rev.keys():\n",
    "    c = Counter([res[_] for _ in keys_rev[k]])\n",
    "    del c[-1]\n",
    "    del c[None]\n",
    "    correct += c.most_common(1)[0][1]\n",
    "    total += sum(c.values())\n",
    "    \n",
    "print(correct / total * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
